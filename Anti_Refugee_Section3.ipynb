{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of StudentCopy_Anti_Refugee_Section3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIIzGUA62ebe"
      },
      "source": [
        "# Project: Anti-Refugee Tweet Classification using Sentiment Analysis\n",
        "---\n",
        "### Goal: Predict whether a given tweet is Pro-Refugee or Anti-Refugee.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OaJiaAs7Dg"
      },
      "source": [
        "Today, we will be building on our earlier insights about models to build more sophisticated models that take into account the semantics of the words in our tweets–not just their counts.\n",
        "\n",
        "## Imports\n",
        "\n",
        "Run the below cells to get started–this will take a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGldMnNXk7Od"
      },
      "source": [
        "#@title Run this to import all the necessary packages { display-mode: \"form\" }\n",
        "import json\n",
        "import tweepy\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.tokenize import TweetTokenizer, sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords' ,quiet=True)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Input\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1ifYLZ-19ZyjjRUICe4PDRmZFAkyL73d0','./source_data.zip',True)\n",
        "my_zip = zipfile.ZipFile('./source_data.zip', mode = 'r')\n",
        "my_zip.extractall()\n",
        "basepath = './drive/Team Drives/Inspirit Curriculum/Inspirit AI Program/Working Materials/Tejit\\'s Material/Anti-Refugee Sentiment Analysis'\n",
        "shutil.move('./Anti-Refugee Sentiment Analysis/', basepath)\n",
        "\n",
        "module_folder = './drive/Team Drives/Inspirit Curriculum/Inspirit AI Program/Working Materials/Tejit\\'s Material/Anti-Refugee Sentiment Analysis/'\n",
        "\n",
        "if module_folder not in sys.path: sys.path.append(module_folder)\n",
        "import lib\n",
        "from lib import Tweet\n",
        "from lib import Tweet_counts\n",
        "\n",
        "# # If the above doesn't work, then upload the file!\n",
        "# from google.colab import files\n",
        "# src = list(files.upload().values())[0]\n",
        "# open('lib.py','wb').write(src)\n",
        "# import lib\n",
        "# from lib import Tweet\n",
        "# from lib import Tweet_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhH19RN2e7CT",
        "outputId": "d3526d73-a424-4170-ae1f-ac01af919022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this to read the data from data.json and split in train and test { display-mode: \"form\" }\n",
        "file_name = 'data.json'\n",
        "dir_name = 'Data'\n",
        "file_path = os.path.join(basepath, dir_name, file_name)\n",
        "data = lib.read_json(file_path, shuffle=True, remove_words = set(stopwords.words('english')))\n",
        "train, test = train_test_split(data, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique tweets read are: 635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R9jrEUMCk7M"
      },
      "source": [
        "# Milestone 1: Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbbTY-Q2jxH"
      },
      "source": [
        "## Introduction - Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKFDPBxpV8k0"
      },
      "source": [
        "### Disadvantage of One-Hot-Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhoP958eQaVq"
      },
      "source": [
        "Previously, we covered Bag of Words and Tfidf that is encoding words as vectors in a matrix. We did this because we cannot pass the words directly into a model and hence required some sort of numerical representations for words. However, there are some disadvantages of using these vectors as a method to encode words. The two main ones are:\n",
        "\n",
        "1. They are too sparse, given a large corpus. A corpus is the vocabulary of words in a text. If there are way too many words in the text, then the length of the matrix in a one hot encoding representation is too large, with many 0's in the matrix. This makes it difficult to store and possibly computationally too expensive to work with.\n",
        "\n",
        "2. These encodings do not encode semantics. Semantics is the branch of linguistics concerned with meaning. If we use one hot encoding, we don't have any information about the meaning of the words in the sentences, which can be defined by its placement in the sentence. For instance, the meaning of the word 'laid' in the two tweets is quite antithetical:\n",
        "\n",
        "> Tweet 1: He laid the foundation to peace #refugeeswelcome<br>\n",
        "> Category: False\n",
        "\n",
        "> Tweet 2: Shot by the thief, the refugee laid on the floor <br>\n",
        "> Category: True\n",
        "\n",
        "\n",
        "You can see why these sparse encodings are not the best method to procceed with. They provide us with a good representation of the tweets and a good basic model, however, we need much more sophisticated models, which can encoding meanings of the words as well.\n",
        "\n",
        "This is where word embeddings comes into play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP0qqEldV_57"
      },
      "source": [
        "### Word2Vec Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YWKzf43hY4O"
      },
      "source": [
        "#### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ3bsn8eVFbu"
      },
      "source": [
        " **Word Embeddings** are a language modeling technique used for mapping words to vectors of real numbers such that semantically similar words have similar vectors. Note that vectors and embeddings are terms often used interchangeably here.\n",
        "\n",
        "The main difference between an embedding vector and a one hot encoded vector is that it is *dense*, i.e. there are no zeroes in the vector. In the previous notebook we saw that the word vector for a single word (whether using Bag of Words or Tfidf for a sentence) was always a one hot encoding, and that it consisted entirely of zeros except for a one in the index of where the word shows up in the vocabulary.\n",
        "This means that every word can be considered to have a \"meaning\" that is entirely independent of all the other words. However we know that this is not true for actual language.\n",
        "\n",
        "At a high level, word embeddings are able to infer the \"meaning\" of words by being trained on a large body of text so that words that co-occur (occur near each other) with similar words have similar embeddings. For example, \"good\" and \"great\" show up near similar words, and so we'd expect them to have similar embeddings. \n",
        "\n",
        "If you're interested in further details of how these work, [read this optional article](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2sq_ZCxiejx"
      },
      "source": [
        "#### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-ilvxLMigjs"
      },
      "source": [
        "Based on the knowledge that you gathered about the Word2Vec model, explain why it might perform better than Bag of words or Tfidf model for anti-refugee sentiment (or any NLP application for that matter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17FfQpozGmdE"
      },
      "source": [
        "### Understanding Word Vectors using *Alice in Wonderland*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cgbWG7TsFMK"
      },
      "source": [
        "To train some of our own embeddings, let us (briefly) forget about anti-refugee sentiment and consider the story of *Alice in Wonderland* as our document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ifkw26Cn7J",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to download all of Alice in Wonderland!\n",
        "def get_alice_data(file_path):\n",
        "    file = open(file_path, 'r')\n",
        "    text = file.read()\n",
        "    text.replace('\\n', ' ')\n",
        "    data = []\n",
        "    for i in sent_tokenize(text):\n",
        "        temp = []\n",
        "        for j in word_tokenize(i): temp.append(j.lower())\n",
        "        data.append(temp)\n",
        "    return data\n",
        "\n",
        "file_name = 'alice.txt'\n",
        "file_path = os.path.join(basepath, file_name)\n",
        "alice_data = get_alice_data(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7mYZGUoMnVc"
      },
      "source": [
        "`alice_data` contains the entire text of *Alice in Wonderland*. Here specifically, it is a list of tokens, i.e. it is a list of all the sentences in *Alice in Wonderland*, and each individual sentence has been tokenized. So `alice_data` is ultimately a list of lists.\n",
        "\n",
        "Change the value of the index in the cell below to see some of these sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5S2pwHIDcC",
        "outputId": "c87efdba-0505-49e2-dec7-6e5d29d52d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "alice_data[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh', 'dear', ',', 'what', 'nonsense', 'i', \"'m\", 'talking', '!', \"'\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1wNQ2yNmHe"
      },
      "source": [
        "Now, we will train word vectors from our Alice in Wonderland text. That is, our model will only have knowledge of the English language from whatever we pass into it from `alice_data`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY_r2uyaDTo4"
      },
      "source": [
        "embedding_model = Word2Vec(alice_data, min_count=1, window=5, hs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyY_OuQELgvm",
        "outputId": "5bcd01da-001f-446a-d762-71bc9144c4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_model.wv['on'].size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MMfnn-cDWm5"
      },
      "source": [
        "The `window` parameter here is the number of words to be selected on either side of the chosen word (the context that determines the numbers in that vector). The `min_count` parameter removes all words that are less that the given count, 1 for instance over here. The details of the `hs` parameter are too advanced for this introductory AI course, suffice to say that by setting it to 1 we allow our models to learn useful representations of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThxRyerNDasb",
        "outputId": "8df9c17c-7038-4cf1-c9e3-70a6c0469465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Word embedding similarities { vertical-output: true, display-mode: \"form\" }\n",
        "word1 = \"green\" #@param {type:'string'}\n",
        "word2 = \"grass\" #@param {type:'string'}\n",
        "\n",
        "print(\"Similarity between \"+word1+\" and \"+word2+\": {:.2f}\".format(embedding_model.wv.similarity(word1,word2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between green and grass: 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6duF06kF9kt"
      },
      "source": [
        "We can see some fairly high similarities between words. You can also try to modify the `min_count` and `window` parameter values and retrain the model to see if that affects any word similarities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t5JOq69j_HK"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhx6nfL0kEpb"
      },
      "source": [
        "Are there any pairs of words that seem to have a higher or lower similarity than you would expect? Why do you think that is the case?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zPYFFChzipX"
      },
      "source": [
        "'''\n",
        "YOUR ANSWER HERE\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-FvXcjwK7gA"
      },
      "source": [
        "We trained our word vectors on a specific piece of text from which it learned what certain words mean. However to gain an understanding of the universal meaning of words training them on larger pieces of text is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxWmPcg9Cq7y"
      },
      "source": [
        "# Milestone 2: Word2Vec Pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssEP3dzeJyFa"
      },
      "source": [
        "## Word2Vec Model 1: Pre-trained Google Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysOqtGZHT2wE"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DI9nFn8ObY7"
      },
      "source": [
        "Let us examine a pretrained Word2Vec model by Google. The below cell will take about 5 min to run, so be patient!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YpxW2O-JRPu",
        "outputId": "531f32fa-e91c-421e-d510-7e4673489bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#@title Run this to load the model { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown The model will take long to load, so do not cancel the process.\n",
        "%%time\n",
        "import zipfile, io, requests\n",
        "print('Downloading...')\n",
        "# Download class resources...\n",
        "r = requests.get(\"https://www.dropbox.com/s/uq9w6e93gccd152/google_news.zip?dl=1\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "print('Loading downloaded files...')\n",
        "file_name = 'GoogleNews-vectors-negative300.bin'\n",
        "dir_name = 'google_news'\n",
        "file_path = os.path.join(dir_name, file_name)\n",
        "model1 = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
        "model1.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "Loading downloaded files...\n",
            "CPU times: user 1min 36s, sys: 15.1 s, total: 1min 51s\n",
            "Wall time: 3min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADIAZQVSmFE"
      },
      "source": [
        "You can see that the array representation of the same word from our pretrained model is much larger than that from our cbow or skip-gram models. This means that our pretrained model contains more *semantic richness* than our quickly trained model on *Alice in Wonderland*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTJjOX4EKIms",
        "cellView": "form",
        "outputId": "e4532aeb-7214-4eeb-ae28-79d29f160d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Run to see the lengths of vectors compared between models\n",
        "print(\"Length of pretrained model word vectors: \" + str(len(model1.get_vector(\"house\"))))\n",
        "print(\"Length of word vectors trained by us: \" + str(len(embedding_model.wv.get_vector(\"house\")))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of pretrained model word vectors: 300\n",
            "Length of word vectors trained by us: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reESQBzHvyPx"
      },
      "source": [
        "Now try changing the words in the interactive cell below to see the similarity between different words. See if these similarities make more sense!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D_xQ7tPvw22",
        "outputId": "1bd7eabc-050d-481b-c47e-ed4a17acbf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Word embedding similarities { vertical-output: true, display-mode: \"form\" }\n",
        "word1 = \"red\" #@param {type:'string'}\n",
        "word2 = \"crimson\" #@param {type:'string'}\n",
        "\n",
        "print(\"Similarity between \"+word1+\" and \"+word2+\": {:.2f}\".format(model1.wv.similarity(word1,word2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between red and crimson: 0.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXPNLtRfydkl"
      },
      "source": [
        "We can actually see what words each of our models thinks is most similar to a given word. The model trained on Alice in Wonderland may give you some strange results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e63MvFAvA1N",
        "cellView": "form",
        "outputId": "8d57c7c6-8aa3-49ae-b2f6-9eb4267067a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#@title Most similar words in different models\n",
        "comparison_word = 'cat'#@param {type: 'string'}\n",
        "alice_words = embedding_model.similar_by_word(comparison_word)\n",
        "full_words = model1.similar_by_word(comparison_word)\n",
        "print(\"MOST SIMILAR WORDS FROM MODEL TRAINED ON ALICE IN WONDERLAND\")\n",
        "for i in alice_words:\n",
        "  print(i[0])\n",
        "  \n",
        "print(\"MOST SIMILAR WORDS FROM MODEL TRAINED ON GOOGLE NEWS\")\n",
        "for i in full_words:\n",
        "  print(i[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOST SIMILAR WORDS FROM MODEL TRAINED ON ALICE IN WONDERLAND\n",
            "talking\n",
            "course\n",
            "'yes\n",
            "meaning\n",
            "'i\n",
            "ready\n",
            "porpoise\n",
            "getting\n",
            "offended\n",
            "'m\n",
            "MOST SIMILAR WORDS FROM MODEL TRAINED ON GOOGLE NEWS\n",
            "cats\n",
            "dog\n",
            "kitten\n",
            "feline\n",
            "beagle\n",
            "puppy\n",
            "pup\n",
            "pet\n",
            "felines\n",
            "chihuahua\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxOZfY4hW07C"
      },
      "source": [
        "#### Setting up the data\n",
        "\n",
        "Now that we have the model in place, let us return to our problem of Tweet anti-refugee sentiment analysis.\n",
        "\n",
        "We can make a list of tokenLists of all training and testing tweets (similar to the list of tokenized sentences from Alice in Wonderland), and also a list of the categories of the training and testing tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mhAow8SVs44"
      },
      "source": [
        "train_tokenized = []\n",
        "train_tweet_label = []\n",
        "for t in train:\n",
        "  train_tokenized.append(t.tokenList) # this is x_train\n",
        "  train_tweet_label.append(t.category) # this is y_train\n",
        "\n",
        "test_tokenized = []\n",
        "test_tweet_label = []\n",
        "for t in test:\n",
        "  test_tokenized.append(t.tokenList) # this is x_test \n",
        "  test_tweet_label.append(t.category) # this is y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJN_3wZNYhWz"
      },
      "source": [
        "### Sentence Embeddings\n",
        "\n",
        "So far we have discussed how to get the meaning of individual words as dense vectors. How would one step that up to get the meaning of a sentence, or a tweet, from these vectors?\n",
        "\n",
        "One way is to simply add all the vectors in a sentence together and divide by the length of the vector. This creates an \"average\" of all the word vectors in the sentence together.\n",
        "\n",
        "For example, if we have three embedding vectors in our sentence with the values \n",
        "\n",
        "`[1 7 3]`  `[2 2 2]` `[3 1 3]`\n",
        "\n",
        "Then our average sentence vector will be `[2 3.33 2.67]`.\n",
        "\n",
        "The `word_averaging_list(model, tokenizedList)` method from the lib file does this for us. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t6sdyMIPs4j"
      },
      "source": [
        "The method uses the trained model and the tokenized input from a list to convert the Tweet to dense vector form using the model. This input, converted, can then be used for prediction or testing. The method takes the model and the list of tokens as the parameter. \n",
        "\n",
        "We need to convert both our test and train data to these dense vectors for each model.\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOsPO6WIYD7W"
      },
      "source": [
        "word2vec_train = lib.word_averaging_list(model1,train_tokenized)\n",
        "word2vec_test = lib.word_averaging_list(model1,test_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npZi7SPxhS2O"
      },
      "source": [
        "The cell below shows us what one of these dense vectors looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tHeyE-Exgfs",
        "outputId": "485c96d8-9e6b-4187-ab55-db842edfb15c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "word2vec_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05240037,  0.05812327, -0.02199082,  0.09079961, -0.00912755,\n",
              "       -0.0213754 ,  0.0073995 , -0.07151303,  0.01555777,  0.04452036,\n",
              "       -0.0464788 , -0.13087013, -0.00986861,  0.07517771, -0.08234611,\n",
              "        0.10848299,  0.07981247,  0.13648252,  0.04324709, -0.03791617,\n",
              "        0.0301124 ,  0.0254908 ,  0.08625544,  0.01739423,  0.14157361,\n",
              "        0.0189876 , -0.0544249 , -0.04449974,  0.01741578, -0.090001  ,\n",
              "       -0.01486013,  0.01701555,  0.0011399 ,  0.00788036,  0.02955163,\n",
              "        0.0211152 ,  0.02558886,  0.03555361,  0.01488241,  0.01644401,\n",
              "        0.13429885, -0.01448205,  0.11589498, -0.01505876, -0.04160787,\n",
              "       -0.03573069, -0.10899768, -0.01976457, -0.0090813 ,  0.06748169,\n",
              "        0.02429601,  0.03884685, -0.00935784,  0.06088878,  0.05445455,\n",
              "        0.05046721, -0.08344578, -0.05054331,  0.03654719, -0.10010935,\n",
              "       -0.09494709,  0.10158836, -0.14050725, -0.057026  ,  0.04732022,\n",
              "       -0.06388194,  0.00282979,  0.02897251, -0.09681907,  0.05605771,\n",
              "        0.0540313 ,  0.05516173,  0.06515332, -0.02543021, -0.12063144,\n",
              "       -0.05854667,  0.02192432,  0.11818888,  0.07242252,  0.01434851,\n",
              "       -0.08964774,  0.05131561, -0.02650996,  0.01937997,  0.07016061,\n",
              "       -0.01409995, -0.05504408,  0.06179394,  0.02068234,  0.0605974 ,\n",
              "        0.08285174,  0.01308833, -0.06066511, -0.03322219,  0.02726543,\n",
              "        0.00145534,  0.01647568, -0.05746541,  0.04483025, -0.04512858,\n",
              "       -0.09394798, -0.0471107 ,  0.00074333,  0.03706233,  0.02552808,\n",
              "       -0.04042432, -0.0304445 , -0.07684916,  0.07636881, -0.0136641 ,\n",
              "       -0.03421537, -0.08471313, -0.01525946,  0.01956112,  0.06260629,\n",
              "        0.01629434,  0.06430474, -0.02662959,  0.00259841,  0.00592621,\n",
              "       -0.04951673,  0.06035075, -0.08432823,  0.03532213,  0.07553444,\n",
              "        0.00805672, -0.12986743, -0.02329401,  0.09275827, -0.06250942,\n",
              "       -0.03075764, -0.04584517, -0.08514452, -0.03701188, -0.15272515,\n",
              "        0.04600038,  0.05042526,  0.11514666,  0.02759493,  0.00248178,\n",
              "       -0.00113912,  0.02415991,  0.04121005, -0.05161569,  0.00952941,\n",
              "        0.00155868, -0.02425219,  0.02239595,  0.0112834 ,  0.01705194,\n",
              "        0.0234255 ,  0.04425652, -0.0315223 ,  0.07633542, -0.00770368,\n",
              "       -0.05485905, -0.07829418, -0.0184061 , -0.06217068, -0.07100093,\n",
              "        0.01027543,  0.10072591,  0.02194929,  0.05013504,  0.05145799,\n",
              "       -0.04368993,  0.02444745, -0.05106324, -0.05525884, -0.01944991,\n",
              "       -0.05480617,  0.00119233, -0.00040001, -0.00721142,  0.02147979,\n",
              "       -0.0435969 ,  0.06930069, -0.10789789, -0.03436184,  0.03180145,\n",
              "       -0.0515756 , -0.07766642,  0.00251303, -0.01358705, -0.05013699,\n",
              "        0.07383485, -0.01466976,  0.05104205,  0.05069914,  0.04011094,\n",
              "       -0.03501391,  0.00500647,  0.07782865, -0.01923467,  0.01514249,\n",
              "       -0.08678878, -0.00626756,  0.01780263, -0.03569974, -0.08309096,\n",
              "       -0.06207379,  0.07867113, -0.06344845, -0.02075791,  0.02714707,\n",
              "       -0.06131024, -0.05628989, -0.01370313, -0.01318165,  0.06098616,\n",
              "       -0.00399019,  0.09922935, -0.08034147, -0.01013434, -0.03953584,\n",
              "        0.04664026,  0.1149059 ,  0.05648308, -0.07182596, -0.05148103,\n",
              "       -0.03575972, -0.09732981, -0.00025718, -0.01333094,  0.03328525,\n",
              "        0.02419221, -0.02575215, -0.02382081,  0.08625305, -0.04183852,\n",
              "        0.040125  , -0.08628308, -0.00881747, -0.03644749,  0.09221876,\n",
              "        0.06890874, -0.01532692, -0.0176467 ,  0.05098566, -0.03404597,\n",
              "        0.01654089,  0.04543958,  0.07288489, -0.07003402, -0.02293101,\n",
              "       -0.00214007, -0.05530644,  0.08175393, -0.06396818, -0.02600578,\n",
              "       -0.00348069,  0.10191769,  0.01254419,  0.03777542,  0.02904672,\n",
              "       -0.05960336, -0.03344163, -0.00282352,  0.05350628, -0.13324536,\n",
              "       -0.04006956,  0.05749869, -0.04744506,  0.09181551, -0.02486987,\n",
              "        0.11268403, -0.03338288,  0.00122493, -0.02750406, -0.03362753,\n",
              "       -0.03227296,  0.08149053,  0.07521489,  0.04107027,  0.00841741,\n",
              "       -0.09139305, -0.07158359, -0.11371794, -0.12685546,  0.08412846,\n",
              "        0.01737085,  0.03453165,  0.00547925,  0.12929446,  0.04073691,\n",
              "        0.02943352, -0.00673169,  0.01708709,  0.08150049,  0.00629512,\n",
              "        0.0104931 , -0.01604506, -0.14114654,  0.00058711,  0.00317057,\n",
              "        0.04268116,  0.0083304 , -0.02864769,  0.00406996, -0.01667421],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgnIORxqaX8G"
      },
      "source": [
        "Now, we will run logistic regression on this transformed input. Instead of calling from `sklearn`, we have made a concise function in the lib package for you guys! \n",
        "\n",
        "You can call the logistic regression function this way:\n",
        "\n",
        "> `logistic_model(train_data, train_labels, test_data, test_labels)`\n",
        "\n",
        "and it takes the following parameters:\n",
        "\n",
        "> train_data: the training vectors or data, this would go in the logistic regression model's fit function directly <br>\n",
        "> train_labels: the training data's labels <br>\n",
        "> test_data: the testing vectors or data, this would go in the logistic regression model's predict function directly <br>\n",
        "> test_labels: the test data's labels\n",
        "\n",
        "and it returns the predicted labels and the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxNKdeScQnB"
      },
      "source": [
        "In this case, we would use it this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX-DNE5zaXaj",
        "outputId": "53f29528-1c4e-46f4-9c1e-b10b5a35957b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "predicted1, matrix1 = lib.logistic_model(word2vec_train, train_tweet_label, word2vec_test, test_tweet_label)\n",
        "predicted1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False,  True,  True, False, False,  True, False,  True,\n",
              "        True, False, False, False,  True,  True,  True,  True,  True,\n",
              "        True,  True, False, False, False, False, False, False, False,\n",
              "       False,  True, False, False,  True,  True, False,  True, False,\n",
              "       False, False,  True,  True,  True, False,  True, False,  True,\n",
              "        True, False, False, False,  True, False, False, False,  True,\n",
              "        True,  True,  True, False, False,  True,  True, False,  True,\n",
              "        True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3-TImFxdn2x"
      },
      "source": [
        "To show the confusion matrix, use the method `disp_confusion_matrix(matrix)` from the lib package, like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP8L0y0Idl3l",
        "outputId": "0e2f770e-a99b-4208-cd2f-eca564d9c7db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "lib.disp_confusion_matrix(matrix1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       True   False\n",
            "True      23      7\n",
            "False      9     25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L83Uen7retGD"
      },
      "source": [
        "To see stats for this prediction, you can use `model_stats(matrix)` method from the lib file. The method takes the confusion matrix as an argument and prints the number of correct and incorrect tweets and the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aen4QwGdz82",
        "outputId": "7dfae667-acfd-454e-bda6-b2ebe8f6d7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "lib.model_stats(matrix1, to_print=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of correct predictions are: 48\n",
            "The total number of incorrect predictions are: 16\n",
            "Accuracy on the test data is: 75.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNtOx-yOf93k"
      },
      "source": [
        "Great! This model is pretty accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vyvCcn8-y-H"
      },
      "source": [
        "Let us also look at the tweets this model classified incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VA48Kh_-2Yb",
        "outputId": "4dd677aa-4e73-40bf-c9b7-94fcd7d17bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print('{:^125}|{:^10}|{:^7}'.format('Tweet','Category','Result'))\n",
        "for i in range(len(test)):\n",
        "  if test_tweet_label[i] != predicted1[i]:\n",
        "    print('{:<125}|{:^10}|{:^7}'.format(str(test[i]), test_tweet_label[i], predicted1[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                            Tweet                                                            | Category |Result \n",
            "yes carrying home heart dvpit                                                                                                |    0     |   1   \n",
            "knew would happen one reason nevertrump                                                                                      |    0     |   1   \n",
            "every refugee term use forced upon communities che                                                                           |    1     |   0   \n",
            "two aftermaths refugee crime cases sweden via                                                                                |    1     |   0   \n",
            "2012-2014 refugee breakdown state altright maga refugeesnotwelcome greatreplacement rapefugees banislam                      |    1     |   0   \n",
            "refugee crimes area country                                                                                                  |    1     |   0   \n",
            "god help poor refugees frump probably send gitmo turnbull dutton give f ck                                                   |    0     |   1   \n",
            "idek say someone even joke innocent child                                                                                    |    0     |   1   \n",
            "idea refugees come need get job make positive contributions society                                                          |    1     |   0   \n",
            "read bible jesus refugee news flashjesus white wait w                                                                        |    0     |   1   \n",
            "horrible iraqi refugee germany found dead forest beaten tied w cables tree last yr 4 racist thugs htt                        |    0     |   1   \n",
            "yes happens suspect people connected refugee charity ngos responsible open borders global scale                              |    1     |   0   \n",
            "make war get refugee                                                                                                         |    1     |   0   \n",
            "refugee thing even feel safe going home                                                                                      |    1     |   0   \n",
            "refugee history expulsion ethnic germans poland czechoslovakia second world war                                              |    0     |   1   \n",
            "come across sea keen share refugee drive                                                                                     |    1     |   0   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKd8AWbsT0HI"
      },
      "source": [
        "### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0nebpQsT4fr"
      },
      "source": [
        "Why do you think the tweets above were classified incorrectly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKdayiRF3BrO"
      },
      "source": [
        "'''\n",
        "idk maybe they share more similarities w the other category\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeKHsKY3_b2a"
      },
      "source": [
        "## Word2Vec Model2 : Pre-trained Twitter Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUuv6emCVwmZ"
      },
      "source": [
        "### Setting up model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT7pMxH9AjTF"
      },
      "source": [
        "The best way forward in practice is to train word vectors on the text data relevant to whichever specific problem you are studying. Ideally training these dense vectors on the relevant text allows the word vectors to pick up whatever nuances are present in that data already present.\n",
        "\n",
        "This following model has been trained on our specific dataset of refugee-related Tweets. Hence, logically, it should be better than the previously trained model! Run the below cell to load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjudR4jQxgG5",
        "outputId": "55f05ab5-93ba-411e-bb94-73faf4974d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Setting up model2 { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown >Setting up the model might take time. <br> <br>\n",
        "#@markdown >You can access the model as model2\n",
        "%%time\n",
        "file_name = 'word2vec_model_twitter.txt'\n",
        "dir_name = 'Word2Vec_models'\n",
        "file_path = os.path.join(basepath, dir_name, file_name)\n",
        "model2 = KeyedVectors.load_word2vec_format(file_path, binary=False) # binary will be false in case of text files\n",
        "model2.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 10s, sys: 2.88 s, total: 2min 13s\n",
            "Wall time: 2min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EU-yYPaVvju"
      },
      "source": [
        "### Twitter Model Predicting - Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXZU2FLWV78F"
      },
      "source": [
        "The way you prepare the data and make the predictions for this model is the same as the previous model. In case you get confused, refer above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhdoKVrVCRoD"
      },
      "source": [
        "Applying the model on train and test tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwtTul7wB7EY"
      },
      "source": [
        "### Your code goes here ###\n",
        "\n",
        "# word2vec_train2 = use lib.word_averaging_list with the arguments model2 and train_tokenized (in the same order)\n",
        "# word2vec_test2 = use lib.word_averaging_list with the arguments model2 and test_tokenized (in the same order)\n",
        "word2vec_train2 = lib.word_averaging_list(model2,train_tokenized)\n",
        "word2vec_test2 = lib.word_averaging_list(model2,test_tokenized)\n",
        "\n",
        "### Your code ends here ###\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OAg5CaYPEN"
      },
      "source": [
        "### Twitter Model Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tOD__PgCWY9"
      },
      "source": [
        "If all went well, the below code should evaluate your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWQhLGVBB_dF",
        "outputId": "59f4b349-1d3c-41de-876c-d41341c9ff94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "predicted2, matrix2 = lib.logistic_model(word2vec_train2, train_tweet_label, word2vec_test2, test_tweet_label)\n",
        "print(predicted2)\n",
        "lib.model_stats(matrix2, to_print=True)\n",
        "lib.disp_confusion_matrix(matrix2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False  True False False  True False  True  True False False\n",
            " False  True  True  True  True  True  True  True False False False False\n",
            " False False False False  True False False  True  True False  True False\n",
            " False False False  True  True False  True False  True False False False\n",
            "  True  True False False False  True False  True  True False False  True\n",
            "  True False  True False]\n",
            "The total number of correct predictions are: 47\n",
            "The total number of incorrect predictions are: 17\n",
            "Accuracy on the test data is: 73.44%\n",
            "       True   False\n",
            "True      25      5\n",
            "False     12     22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1V7cOMW_zmZ"
      },
      "source": [
        "Let us also look at the tweets this model classified incorrectly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIAjkOr4_zma",
        "outputId": "2a6e28e6-e7cb-425d-828a-21975dc9bf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print('{:^125}|{:^10}|{:^7}'.format('Tweet','Category','Result'))\n",
        "for i in range(len(test)):\n",
        "  if test_tweet_label[i] != predicted2[i]:\n",
        "    print('{:<125}|{:^10}|{:^7}'.format(str(test[i]), test_tweet_label[i], predicted2[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                            Tweet                                                            | Category |Result \n",
            "germany 82 year-old woman tortured death syrian child refugee                                                                |    1     |   0   \n",
            "believe muslim molesting manikin nothing sacred quality refugee                                                              |    1     |   0   \n",
            "yes carrying home heart dvpit                                                                                                |    0     |   1   \n",
            "knew would happen one reason nevertrump                                                                                      |    0     |   1   \n",
            "every refugee term use forced upon communities che                                                                           |    1     |   0   \n",
            "two aftermaths refugee crime cases sweden via                                                                                |    1     |   0   \n",
            "2012-2014 refugee breakdown state altright maga refugeesnotwelcome greatreplacement rapefugees banislam                      |    1     |   0   \n",
            "refugee crimes area country                                                                                                  |    1     |   0   \n",
            "god help poor refugees frump probably send gitmo turnbull dutton give f ck                                                   |    0     |   1   \n",
            "idek say someone even joke innocent child                                                                                    |    0     |   1   \n",
            "idea refugees come need get job make positive contributions society                                                          |    1     |   0   \n",
            "horrible iraqi refugee germany found dead forest beaten tied w cables tree last yr 4 racist thugs htt                        |    0     |   1   \n",
            "yes happens suspect people connected refugee charity ngos responsible open borders global scale                              |    1     |   0   \n",
            "refugee crisis intentionally used infiltrate bring western civilization                                                      |    1     |   0   \n",
            "refugee thing even feel safe going home                                                                                      |    1     |   0   \n",
            "come across sea keen share refugee drive                                                                                     |    1     |   0   \n",
            "muhammad refugee taken jewish town medina within 5 years driven executed enslaved e                                          |    1     |   0   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inV3ZAQVYVlb"
      },
      "source": [
        "### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35NOTAQ_YVlc"
      },
      "source": [
        "Why do you think the tweets above were classified incorrectly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6r7AoZObRQy"
      },
      "source": [
        "## Word2Vec Models: Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFwvDqOadPEg"
      },
      "source": [
        "### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BChl2VPhhoZG"
      },
      "source": [
        "Why do you think one model was better than the other?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_dWmxWG3hv1"
      },
      "source": [
        "'''\n",
        "YOUR ANSWER HERE\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NNPWE2EDzIp"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r82e-tELEEDU"
      },
      "source": [
        "Congratulations on completing the notebook for this project! Although we have gone deep into specifics of what types of representations work best for words, we are still trying to solve a real world problem. From the text data we have, and with basic classification models, we cannot classify anti-refugee tweets from pro refugee tweets with a higher accuracy than about 82-83%. Detecting the true meaning, or the intention of what someone is saying involves a lot more information than just the words they use. \n",
        "\n",
        "The best way to determine someone's true intention is to assess the context in which they say something. In the case of anti-refugee Tweets, in order to better classify a Tweet, we would need information about the user as well. Their previous Tweets, their followers, the other users they follow, and their location may give us some indication of their true interests and their political leanings, and thereby help to tell us whether a particular Tweet truly does express anti-refugee sentiment (or any kind of hate speech). \n",
        "\n",
        "This is not an easy problem to solve. Big tech companies are trying their best to help as much as they can, but in the end this is an expression of issues that are deeper than technology and social media. The main fixes to these expressions of anti-refugee sentiment will be geopolitical and humanitarian in nature."
      ]
    }
  ]
}