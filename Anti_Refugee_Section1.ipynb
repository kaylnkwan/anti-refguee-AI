{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of StudentCopy_Anti_Refugee_Section1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApwMhgQgwtZm"
      },
      "source": [
        "# Identifying Anti-Refugee Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPz-x3shxgEA"
      },
      "source": [
        "## Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAiQgufXVlC5"
      },
      "source": [
        "**Sentiment Analysis**<br>\n",
        "It is the process of using the computer to identify and categorize opinions expressed in a piece of text in order to determine whether the writer's attitude towards the given topic is positive or negative (or sometimes even neutral). It can also reveal their emotional state, and the intended effect of their words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6-tuOw1Vmsd"
      },
      "source": [
        "**Why conduct sentiment analysis ?**<br>\n",
        "The answer depends on where the tool is applied! In business, it can be used to predict the sentiment of the consumers in a market, thereby aiding the growth of the company. In politics, the sentiments of the voters can be used to determine the most appropriate strategy. By listening to and analysing comments on Facebook and Twitter, local government departments can gauge public sentiment and use the results to improve services they provide to the public. Universities can use sentiment analysis to analyze student feedback and improve their curriculum. These are a few of the many uses of sentiment analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFLYr9neVoVO"
      },
      "source": [
        "**What is Anti-Refugee Tweet Classification**<br>\n",
        "Anti-refugee tweet classification, the topic that we would be covering in the coming few days, is classifying a given tweet as pro-refugee or anti-refugee. An example to illustrate the definition:\n",
        "\n",
        "> *anti-refugee tweet*: 'muslim refugee charged with beating a woman'<br>\n",
        "> *pro-refugee tweet*: 'refugee hotspots in italy and greece not yet adequate'\n",
        "\n",
        "As you can guess from the above example, an anti-refugee tweet would have negative words, and would convey negative sentiments towards the refugees, sentiments that would potray the refugee in a negative light, while the converse is true for pro-refugee tweets. **Understanding anti-refugee sentiment is the first step in addressing it.** This project will allow us to use AI models to do so. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o2HzoGmw2rw"
      },
      "source": [
        "# Milestone 1: Exploring our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyCn-CT4aVPi"
      },
      "source": [
        "#@title Run this to import all the necessary packages. This will take a few minutes! { display-mode: \"form\" }\n",
        "import json\n",
        "import tweepy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords' ,quiet=True)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1ifYLZ-19ZyjjRUICe4PDRmZFAkyL73d0','./source_data.zip',True)\n",
        "my_zip = zipfile.ZipFile('./source_data.zip', mode = 'r')\n",
        "my_zip.extractall()\n",
        "basepath = './drive/Team Drives/Inspirit Curriculum/Inspirit AI Program/Working Materials/Tejit\\'s Material/Anti-Refugee Sentiment Analysis'\n",
        "try: \n",
        "  shutil.move('./Anti-Refugee Sentiment Analysis/', basepath)\n",
        "except shutil.Error:\n",
        "  pass\n",
        "\n",
        "\n",
        "module_folder = './drive/Team Drives/Inspirit Curriculum/Inspirit AI Program/Working Materials/Tejit\\'s Material/Anti-Refugee Sentiment Analysis/'\n",
        "if module_folder not in sys.path: sys.path.append(module_folder)\n",
        "import lib\n",
        "from lib import Tweet\n",
        "from lib import Tweet_counts\n",
        "\n",
        "# # If the above doesn't work, then upload the file!\n",
        "# from google.colab import files\n",
        "# src = list(files.upload().values())[0]\n",
        "# open('lib.py','wb').write(src)\n",
        "# import lib\n",
        "# from lib import Tweet\n",
        "# from lib import Tweet_counts\n",
        "\n",
        "from lib import *\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeuFtjtEX_Xj"
      },
      "source": [
        "### Understanding the structure of a tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT1SRaYJYEkz"
      },
      "source": [
        "Tweets are composed of:\n",
        "* Hashtags: Keywords that start with the '#' symbol\n",
        "* Mentions: Referencing another user/person with '@'\n",
        "* Everything else: Anything that isn't a hashtag or mention!\n",
        "\n",
        "We've made a convenient interface for processing our tweets, which we call the `Tweet` class. Let's try out the `Tweet` class!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIbrGhTBYgSz"
      },
      "source": [
        "my_tweet = Tweet('these are #tags and this is a @mention. hey #wait there\\'s another @one here too','true') \n",
        "# takes in text and true or false - don't worry about true or false right now!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp7lOMlEYrYc",
        "outputId": "617990e4-2fd8-43da-bdc8-adc6790a3652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check out the hashtags!\n",
        "my_tweet.hashtags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#tags', '#wait']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgqZj80BYyht",
        "outputId": "d6d1103e-f1c1-4793-c52a-3a6714e7362c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check out the mentions!\n",
        "my_tweet.mentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@mention', '@one']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM_dl10OY2P1",
        "outputId": "53ab2f1d-5bb6-4501-a306-d619598ac78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# check out the tweet text!\n",
        "my_tweet.tokenList"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['these',\n",
              " 'are',\n",
              " 'tags',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'hey',\n",
              " 'wait',\n",
              " \"there's\",\n",
              " 'another',\n",
              " 'here',\n",
              " 'too']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYYqX5Cmw9zn"
      },
      "source": [
        "## Activity 1. Examining our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7Rr9DKGXb4"
      },
      "source": [
        "Let's a take at our prebuilt database of tweets extracted from twitter! It is in a folder called Data and is stored in a file called data.json."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvg9h7gnEi7S"
      },
      "source": [
        "file = open(basepath+'/Data/data.json','r')\n",
        "data = json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEi1B4l5EoHT"
      },
      "source": [
        "Our `data` is a list of tweets that are classified as either TRUE (anti-refugee) or FALSE (pro-refugee). \n",
        "\n",
        "How many tweets do we have? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tLr5VHTErLr",
        "outputId": "95495628-cdb5-455d-f91d-017728e3bf1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "689"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH4v0oJ7EQ9e"
      },
      "source": [
        "What does each data point look like? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KVsgAoHEJWq",
        "outputId": "9e1ec0e9-39b7-4dd5-ef4e-4bbc0363f17a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data[50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 'TRUE',\n",
              " 'tweet': '#flynn boycott and call out !  chobani yogurt founder pushing for more refugee labor  via @100percfedup'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BmfgwSIE21C"
      },
      "source": [
        "Each data point is a dictionary (in particular, it is a json object) with two keys:\n",
        "\n",
        "* classification: which is the category of the tweet \n",
        "* tweet: which is the tweet text\n",
        "\n",
        "We can access the value in each dictionary element by using the individual key associated with each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9o_YjOZOQnA",
        "outputId": "9eaa08e3-0704-48fc-f0e8-dde3b612616f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# use the 'classification' key to see the sentiment of a tweet\n",
        "data[0]['classification']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TRUE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWxLAKJpFfTs"
      },
      "source": [
        "We need to find an efficient way to split our data into two different groups. One way to do this would be to write a for loop to go through every tweet in the list. But a better way would be to use a list comprehension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mddDvW3PAd_i"
      },
      "source": [
        "### Exercise (Coding): List Comprehensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfN5plIaAnMy"
      },
      "source": [
        "Python provides a handy tool called *list comprehension*. This can allow us to perform the work of a `for` loop and an `if-else` statment in a single line of code. List comprehensions come in handy when parsing through large amouts of data.\n",
        "\n",
        "The cell below reminds us what a `for` loop in Python looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKeidLB5DYRq",
        "outputId": "837343df-98c9-4e0b-83d1-6c138d7f2a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# define our original list\n",
        "original_list = [1,2,9,10,11]\n",
        "\n",
        "# initialize a new list\n",
        "new_list = []\n",
        "\n",
        "# add elements to it!\n",
        "for i in original_list:\n",
        "  new_list.append(i+2)\n",
        "  \n",
        "print(new_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 4, 11, 12, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an-KcgZgEB48"
      },
      "source": [
        "A list comprehension can compress this into one line like: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q8EYrEtEMTO",
        "outputId": "775d521f-6cea-4135-b35f-6441529c69b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_list = [i+2 for i in original_list]\n",
        "print(new_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 4, 11, 12, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh6Nug6zElf3"
      },
      "source": [
        "We can even add conditionals to this! For example..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bz9i7CVE8m6",
        "outputId": "7481ec5d-3148-4b04-de7f-2faac174c346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_list = [i for i in original_list if i > 3]\n",
        "print(new_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9, 10, 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtFkzHEPFNmy"
      },
      "source": [
        "Here, we found all elements in our original list that were greater than 3. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri86Nl_IFW0U"
      },
      "source": [
        "**Now, you try!** \n",
        "\n",
        "Use a list comprehension to get a list of all elements in `original_list` that are less than 10!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ctpLZmCFy2Q",
        "outputId": "4c8f5c3b-48cd-44b2-aca0-0e5c581cebfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### YOUR CODE HERE\n",
        "new_list = [i for i in original_list if i < 10]\n",
        "print(new_list)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06WEWVnKXTE"
      },
      "source": [
        "Now that we know how list comprehensions and dictionaries work, we can try using them to split our data into two separate lists. Remember, a list comprehension is essentially a `for` loop and an `if` condition in a single line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YBdb8MiPQNU",
        "outputId": "ffd845fa-67ab-4fca-9183-bc2a0f9e7fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### YOUR CODE HERE\n",
        "pro = [d['tweet'] for d in data if d['classification'].lower() == 'false']\n",
        "anti = [d['tweet'] for d in data if d['classification'].lower() == 'true']\n",
        "\n",
        "for x in data:\n",
        "  print(x['classification'].lower())\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "true\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "true\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "false\n",
            "true\n",
            "true\n",
            "false\n",
            "true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZxWWYZQFS6H"
      },
      "source": [
        "How many tweets do we have of each class? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ZioTerAHUK",
        "outputId": "704477dc-bb3d-4201-bba5-219de0ce319c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# how many pro refugee tweets? \n",
        "len(pro)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kvTIYAAQ8K",
        "outputId": "c2e3191a-0e75-4b9d-c5c4-126d3dd1738d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# how many anti refugee tweets?\n",
        "len(anti)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "343"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9bGJsKngPDc",
        "outputId": "e94f56b7-1fec-4e96-b2ff-93081c221852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pro[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rt @rangersfanjoe @timpimley @foxnews @cristinacorbin they were .  starbucks has hired 10000 vets since 2014 .  well before the refugee progr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiPdxAtRxDKX"
      },
      "source": [
        "# Milestone 2: Handmade classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv-3ukngRL7V"
      },
      "source": [
        "We'll actually split our data into lists of `Tweet` objects so we can access the hashtags and mentions easily. \n",
        "\n",
        "Let's now split our data into a full data set (`tweet_data`). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKHfczJCbZEA"
      },
      "source": [
        "#@title Formatting our data! {display-mode: 'form'}\n",
        "tweet_data   = [Tweet(t['tweet'], t['classification']) for t in data]\n",
        "pro_tweets     = [Tweet(t['tweet'], t['classification']) for t in data if t['classification'].lower()=='false']\n",
        "anti_tweets    = [Tweet(t['tweet'], t['classification']) for t in data if t['classification'].lower()=='true']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GqZALPXQNRS",
        "outputId": "e4a076a2-c439-48f5-93aa-68f497086c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(pro_tweets[0])\n",
        "print(pro_tweets[0].mentions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rt they were starbucks has hired 10000 vets since 2014 well before the refugee progr\n",
            "['@rangersfanjoe', '@timpimley', '@foxnews', '@cristinacorbin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWywh1DehXyv",
        "outputId": "9d931813-47d6-4416-c5d4-34beb4b3a097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pro_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QroVN-LwIbu"
      },
      "source": [
        "## Activity 2a. Looking at pro vs. anti tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e4gKLsoGuAN"
      },
      "source": [
        "### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtpmMf9qxE5J"
      },
      "source": [
        "We reformated our data into lists of 'tweets'. Let's now look at a few pro and anti refugee tweets, their original text, hashtags, and mentions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i4dnkIwwPea",
        "outputId": "d0ab2e28-8d7e-4d3c-cd27-b9d8e524df3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# display 5 pro refugee tweets!\n",
        "for i in range(5):\n",
        "  this_tweet = pro_tweets[i]\n",
        "  print('---Original tweet text---')\n",
        "  print(this_tweet.original_tweet_text)\n",
        "\n",
        "  print('---Hashtags---')\n",
        "  print(this_tweet.hashtags)\n",
        "\n",
        "  print('---Mentions---')\n",
        "  print(this_tweet.mentions)\n",
        "  \n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Original tweet text---\n",
            "rt @rangersfanjoe @timpimley @foxnews @cristinacorbin they were .  starbucks has hired 10000 vets since 2014 .  well before the refugee progr\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@rangersfanjoe', '@timpimley', '@foxnews', '@cristinacorbin']\n",
            "\n",
            "\n",
            "---Original tweet text---\n",
            "rt @refugeeinfobus another unaccompanied refugee child arrested in #calais tonight at a evening food distribution\n",
            "---Hashtags---\n",
            "['#calais']\n",
            "---Mentions---\n",
            "['@refugeeinfobus']\n",
            "\n",
            "\n",
            "---Original tweet text---\n",
            "rt @slade now there\\'s an actual deadline .    please continue to shout raise awareness donate to lgbt refugee causes contact\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@slade']\n",
            "\n",
            "\n",
            "---Original tweet text---\n",
            "rt @gisellalomax helping refugees to thrive not just survive - a new approach on responding to refugee crises .\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@gisellalomax']\n",
            "\n",
            "\n",
            "---Original tweet text---\n",
            "rt @yemmadelrey repeat after me i need to watch this video every morning if my ungrateful ass ever think of skipping class &amp takin\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@yemmadelrey']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFG9Th5-wREI",
        "outputId": "555829f0-2457-47cc-d039-d8401922138d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# display 5 anti-refugee tweets!\n",
        "for i in range(5):\n",
        "  this_tweet = anti_tweets[i]\n",
        "  print('---Original text---')\n",
        "  print(this_tweet.original_tweet_text)\n",
        "\n",
        "  print('---Hashtags---')\n",
        "  print(this_tweet.hashtags)\n",
        "\n",
        "  print('---Mentions---')\n",
        "  print(this_tweet.mentions)\n",
        "  \n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Original text---\n",
            "rt @_makada_ muslim refugee charged with beating ga woman with american flag skips court\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@_makada_']\n",
            "\n",
            "\n",
            "---Original text---\n",
            "rt @_makada_ muslim refugee charged with beating ga woman with american flag skips court\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@_makada_']\n",
            "\n",
            "\n",
            "---Original text---\n",
            "rt @johnkstahlusa there\\'s something wrong with this refugee nonsense .  real men stay and fight for their values and country .  #tcot\n",
            "---Hashtags---\n",
            "['#tcot']\n",
            "---Mentions---\n",
            "['@johnkstahlusa']\n",
            "\n",
            "\n",
            "---Original text---\n",
            "trouble is it\\'s all dem-friendly spending  planned parenthood refugee resettlement continuing bribe to obamacar\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "[]\n",
            "\n",
            "\n",
            "---Original text---\n",
            "rt @amike4761 muslim refugees decline work say its against their religion to perform labor for americans .  deport them all ?\n",
            "---Hashtags---\n",
            "[]\n",
            "---Mentions---\n",
            "['@amike4761']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IXWRvyOHHKs"
      },
      "source": [
        "**In your group, discuss:** \n",
        "Does a tweet always have a hashtag or mention? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8WhbA7mMfV3"
      },
      "source": [
        "## Activity 2b. Handmade Rules for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69omzTn4Mh1p"
      },
      "source": [
        "Rule based classification uses certain rules, defined by the user, to classify tweets to the given categories. These rules are generally rigid and hence a rule based classifier cannot assign a probability to a tweet but can only assign a category to it.\n",
        "\n",
        "An example of a rule based classifier is:\n",
        "\n",
        "> If the word 'potato' or 'spinach' occurs in a tweet, then classify the tweet as vegetable, otherwise classify it as a fruit!\n",
        "\n",
        "Oftentimes, due to the rigidity and simplicity of the rule based classifier, the classification is faulty. Hence, do not expect a high accuracy from this classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLS_mIMPGwqK"
      },
      "source": [
        "Before we begin making our rule based classifier, let us visualize the data. Visualization helps us understand properties of the data which will, in turn, help us with the rule based classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avj_MLpryNlT"
      },
      "source": [
        "### Exercise (Discussion): Figuring out the rules for our tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhCoFODIbpSB"
      },
      "source": [
        "\n",
        "\n",
        "Rule based classification, as the name suggests, is based on a given set of rules. In case of tweets, these rules can be a lot of things. Let us look at the data to figure out the things that we can use for rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo7WM9tdcFyd"
      },
      "source": [
        "We know that we have the following unique things in tweets:\n",
        "\n",
        "1. Hashtags\n",
        "2. Mentions\n",
        "3. Other words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfdjDQIBgunT"
      },
      "source": [
        "**Question:** You think hashtags can be used to classify tweets? Give 5 examples of hashtags that can tell pro or anti refugee tweets apart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Tq_IYZb7nM",
        "outputId": "ac3d329d-13df-4474-eff1-dd156340e27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# display the first 10 pro hashtags\n",
        "\n",
        "pro_hashtags = []\n",
        "\n",
        "for tweet in pro_tweets:\n",
        "  if len(tweet.hashtags) > 0: # we use this condition because some tweets might not have hashtags\n",
        "    pro_hashtags.append(tweet.hashtags)\n",
        "  if len(pro_hashtags) == 10:\n",
        "    break\n",
        "\n",
        "\n",
        "pro_hashtags    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#calais'],\n",
              " ['#rohingya'],\n",
              " ['#refugeeswelcome', '#uk'],\n",
              " ['#armeniangenocide', '#refugees'],\n",
              " ['#romarefugees'],\n",
              " ['#refugeeswelcome', '#immigration'],\n",
              " ['#refugeeswelcome', '#immigration'],\n",
              " ['#origami'],\n",
              " ['#tech', '#socialgood'],\n",
              " ['#syrian', '#rap', '#homsies']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmeF-BpncIhW",
        "outputId": "52f87054-9f79-46a6-a95b-87206d24ab61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# display the first 10 anti hashtags\n",
        "### YOUR CODE HERE\n",
        "anti_hashtags = []\n",
        "\n",
        "for tweet in anti_tweets:\n",
        "  if len(tweet.hashtags) > 0: # we use this condition because some tweets might not have hashtags\n",
        "    anti_hashtags.append(tweet.hashtags)\n",
        "  if len(anti_hashtags) == 10:\n",
        "    break\n",
        "\n",
        "\n",
        "anti_hashtags    \n",
        "  \n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['#tcot'],\n",
              " ['#worldpenguinday'],\n",
              " ['#refugees'],\n",
              " ['#refugee', '#travelban'],\n",
              " ['#tuesdaymotivation'],\n",
              " ['#stephaniedavis'],\n",
              " ['#aid4yemen'],\n",
              " ['#flynn'],\n",
              " ['#worldp'],\n",
              " ['#worldpenguinday']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmIlE-E0hAKo"
      },
      "source": [
        "**Question:** Can mentions (tags - '@') be used to classify tweets? Give 5 examples of mentions that can classify pro or anti tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYPCycwPcFWN",
        "outputId": "761a176d-1c66-4bc6-eca2-b802d4b8ead7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# display the first 10 pro mentions\n",
        "\n",
        "pro_mentions = []\n",
        "\n",
        "\n",
        "for tweet in pro_tweets:\n",
        "  if len(tweet.mentions) > 0:\n",
        "    pro_mentions.append(tweet.mentions)\n",
        "  if len(pro_mentions) == 10:\n",
        "    break\n",
        "\n",
        "\n",
        "pro_mentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['@rangersfanjoe', '@timpimley', '@foxnews', '@cristinacorbin'],\n",
              " ['@refugeeinfobus'],\n",
              " ['@slade'],\n",
              " ['@gisellalomax'],\n",
              " ['@yemmadelrey'],\n",
              " ['@gaystarnews'],\n",
              " ['@jonnelledge', '@davidclewis'],\n",
              " ['@rejialex7'],\n",
              " ['@mahyadt', '@thepromisefilm'],\n",
              " ['@makewomencount']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-wgBwSPcSWu",
        "outputId": "a850649d-04ed-4c5a-fe4f-1dcf53a7ed95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# display the first 10 anti mentions\n",
        "### YOUR CODE HERE\n",
        "\n",
        "anti_mentions = []\n",
        "\n",
        "\n",
        "for tweet in anti_tweets:\n",
        "  if len(tweet.mentions) > 0:\n",
        "    anti_mentions.append(tweet.mentions)\n",
        "  if len(anti_mentions) == 10:\n",
        "    break\n",
        "\n",
        "\n",
        "anti_mentions\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['@_makada_'],\n",
              " ['@_makada_'],\n",
              " ['@johnkstahlusa'],\n",
              " ['@amike4761'],\n",
              " ['@100percfedup'],\n",
              " ['@pamelageller'],\n",
              " ['@refugeewatcher'],\n",
              " ['@newttrump'],\n",
              " ['@100percfedup'],\n",
              " ['@naji1290', '@antonioguterres']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUzq0MNehEAr"
      },
      "source": [
        "**Question:** You think any other words from a tweet can be used to classify pro or anti refugee sentiment? Give 5 examples of words that can classify pro or anti tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDZWlT4mcF9e",
        "outputId": "17c936eb-506c-4766-a0de-77b791e01686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# display words in the first 10 pro tweets\n",
        "### YOUR CODE HERE\n",
        "pro_words = []\n",
        "\n",
        "for tweet in pro_tweets:\n",
        "  tokens = [t for t in tweet.tokenList if t not in stopwords.words('english')]\n",
        "  pro_words.append(tokens)\n",
        "  if len(pro_words) == 10:\n",
        "      break  \n",
        "      \n",
        "pro_words\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['rt',\n",
              "  'starbucks',\n",
              "  'hired',\n",
              "  '10000',\n",
              "  'vets',\n",
              "  'since',\n",
              "  '2014',\n",
              "  'well',\n",
              "  'refugee',\n",
              "  'progr'],\n",
              " ['rt',\n",
              "  'another',\n",
              "  'unaccompanied',\n",
              "  'refugee',\n",
              "  'child',\n",
              "  'arrested',\n",
              "  'calais',\n",
              "  'tonight',\n",
              "  'evening',\n",
              "  'food',\n",
              "  'distribution'],\n",
              " ['rt',\n",
              "  'actual',\n",
              "  'deadline',\n",
              "  'please',\n",
              "  'continue',\n",
              "  'shout',\n",
              "  'raise',\n",
              "  'awareness',\n",
              "  'donate',\n",
              "  'lgbt',\n",
              "  'refugee',\n",
              "  'causes',\n",
              "  'contact'],\n",
              " ['rt',\n",
              "  'helping',\n",
              "  'refugees',\n",
              "  'thrive',\n",
              "  'survive',\n",
              "  'new',\n",
              "  'approach',\n",
              "  'responding',\n",
              "  'refugee',\n",
              "  'crises'],\n",
              " ['rt',\n",
              "  'repeat',\n",
              "  'need',\n",
              "  'watch',\n",
              "  'video',\n",
              "  'every',\n",
              "  'morning',\n",
              "  'ungrateful',\n",
              "  'ass',\n",
              "  'ever',\n",
              "  'think',\n",
              "  'skipping',\n",
              "  'class',\n",
              "  'amp',\n",
              "  'takin'],\n",
              " ['government',\n",
              "  'said',\n",
              "  'take',\n",
              "  'disabled',\n",
              "  'refugee',\n",
              "  'children',\n",
              "  'people',\n",
              "  'actually',\n",
              "  'think',\n",
              "  'keep',\n",
              "  'evil',\n",
              "  'fucks'],\n",
              " ['focus',\n",
              "  'receiving',\n",
              "  'countries',\n",
              "  'imho',\n",
              "  'list',\n",
              "  'betts',\n",
              "  'ampp',\n",
              "  'collier',\n",
              "  'fix',\n",
              "  'worlds',\n",
              "  'refugee',\n",
              "  'system'],\n",
              " ['walk', 'mile', 'rohingya', 'refugee', 'shoes', 'new', 'app'],\n",
              " ['rt',\n",
              "  'raped',\n",
              "  'abused',\n",
              "  '23',\n",
              "  'year-old',\n",
              "  'gay',\n",
              "  'refugee',\n",
              "  'bangladesh',\n",
              "  'run',\n",
              "  'nepal',\n",
              "  'shares',\n",
              "  'story'],\n",
              " ['women',\n",
              "  'refugee',\n",
              "  'day',\n",
              "  'sat',\n",
              "  'may',\n",
              "  '6th',\n",
              "  '700',\n",
              "  '430',\n",
              "  'p',\n",
              "  'women',\n",
              "  'saving',\n",
              "  'grace',\n",
              "  'given',\n",
              "  'amazing']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Kamwx6cWMY",
        "outputId": "d3e2affc-6b1d-4980-8719-d77fc479258c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# display words in the first 10 anti tweets\n",
        "### YOUR CODE HERE\n",
        "anti_words = []\n",
        "\n",
        "for tweet in anti_tweets:\n",
        "  tokens = [t for t in tweet.tokenList if t not in stopwords.words('english')]\n",
        "  anti_words.append(tokens)\n",
        "  if len(anti_words) == 10:\n",
        "      break  \n",
        "      \n",
        "anti_words\n",
        "### END CODE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['rt',\n",
              "  'muslim',\n",
              "  'refugee',\n",
              "  'charged',\n",
              "  'beating',\n",
              "  'ga',\n",
              "  'woman',\n",
              "  'american',\n",
              "  'flag',\n",
              "  'skips',\n",
              "  'court'],\n",
              " ['rt',\n",
              "  'muslim',\n",
              "  'refugee',\n",
              "  'charged',\n",
              "  'beating',\n",
              "  'ga',\n",
              "  'woman',\n",
              "  'american',\n",
              "  'flag',\n",
              "  'skips',\n",
              "  'court'],\n",
              " ['rt',\n",
              "  'something',\n",
              "  'wrong',\n",
              "  'refugee',\n",
              "  'nonsense',\n",
              "  'real',\n",
              "  'men',\n",
              "  'stay',\n",
              "  'fight',\n",
              "  'values',\n",
              "  'country',\n",
              "  'tcot'],\n",
              " ['trouble',\n",
              "  'dem-friendly',\n",
              "  'spending',\n",
              "  'planned',\n",
              "  'parenthood',\n",
              "  'refugee',\n",
              "  'resettlement',\n",
              "  'continuing',\n",
              "  'bribe',\n",
              "  'obamacar'],\n",
              " ['rt',\n",
              "  'muslim',\n",
              "  'refugees',\n",
              "  'decline',\n",
              "  'work',\n",
              "  'say',\n",
              "  'religion',\n",
              "  'perform',\n",
              "  'labor',\n",
              "  'americans',\n",
              "  'deport'],\n",
              " ['boycott',\n",
              "  'call',\n",
              "  'chobani',\n",
              "  'yogurt',\n",
              "  'founder',\n",
              "  'pushing',\n",
              "  'refugee',\n",
              "  'labor',\n",
              "  'via',\n",
              "  'worldpenguinday'],\n",
              " ['islam', 'religion', 'bloodthirsty', 'fanaticism'],\n",
              " ['gee',\n",
              "  'even',\n",
              "  'carry',\n",
              "  'olympic',\n",
              "  'torch',\n",
              "  'without',\n",
              "  'muslim',\n",
              "  'nutjobs',\n",
              "  'trying',\n",
              "  'shut',\n",
              "  'freedom'],\n",
              " ['rt',\n",
              "  'geller',\n",
              "  'another',\n",
              "  'official',\n",
              "  'coverup',\n",
              "  'refugee',\n",
              "  'crime',\n",
              "  'breitbart'],\n",
              " ['rt',\n",
              "  'chobani',\n",
              "  'hires',\n",
              "  'refugees',\n",
              "  'id',\n",
              "  'town',\n",
              "  'child',\n",
              "  'refugee',\n",
              "  'sex',\n",
              "  'assault',\n",
              "  'case',\n",
              "  'chobani',\n",
              "  'sues',\n",
              "  'alex',\n",
              "  'jones',\n",
              "  'linking']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0N-9l31dCsa"
      },
      "source": [
        "The more often a hashtag, mention, or rule comes in one category over another, the better we may expect it to work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hik5gPU-paCf"
      },
      "source": [
        "\n",
        "**Play around with the interactive form below to see the count of a given property (i.e. hashtag, mention, or just a word), and how often it shows up in the pro or anti refugee tweets. This may give you some indication of what specific ones might work better to categorize tweets.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxSjaBlstM4z",
        "outputId": "45f67361-ff33-4933-933c-c2e6ae5c5327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Query { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "examine_tweet = Tweet_counts(tweet_data) \n",
        "\n",
        "prop = 'Hashtags' #@param [\"Hashtags\", \"Mentions\", \"Word\"]\n",
        "string = 'buildthewall' #@param {type:\"string\"}\n",
        "\n",
        "if prop=='Hashtags':\n",
        "  if string[0]!= '#': string = '#' + string\n",
        "  print(examine_tweet.query_hashtag(string.lower()))\n",
        "elif prop=='Mentions':\n",
        "  if string[0]!='@': string = '@' + string\n",
        "  print(examine_tweet.query_mentions(string.lower()))\n",
        "elif prop=='Word':\n",
        "  print(examine_tweet.query_words(string.lower()))\n",
        "\n",
        "#@markdown Metions are tags in twitter - @blah, @realdonaldtrump. \n",
        "#@markdown <br><br>**Code result**:\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'pro': 0, 'anti': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0jyeCCMftrU"
      },
      "source": [
        "**When you're happy with your lists, discuss with your instructor, then write your hashtags, mentions, and words in the lists below. These will be the lists you'll be using today to classify the tweets as anti or pro refugee!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKKb1XBB0Oa-"
      },
      "source": [
        "pro_hashtags = ['#buildthewall','#']\n",
        "anti_hashtags = ['#','#']\n",
        "pro_mentions = ['@realdonaldtrunp','@']\n",
        "anti_mentions = ['@','@']\n",
        "pro_words = ['']\n",
        "anti_words = ['']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqYsayzwL9Pe"
      },
      "source": [
        "#@title Instructor Solution {display-mode:'form'}\n",
        "\n",
        "# note to instructor: these won't work very well :( \n",
        "\n",
        "pro_hashtags = ['#rohingya','#refugeeswelcome','#syria','#southsudan']\n",
        "anti_hashtags = ['#maga','#boycottchobani','#buildthewall','#muslim']\n",
        "pro_mentions = ['@appgrefugees','@refugeesintl','@refugeecouncil','@amnestyuk']\n",
        "anti_mentions = ['@realdonaldtrump','@potus','@youtube','@infowars']\n",
        "pro_words = ['syrian','children','crisis','education']\n",
        "anti_words = ['muslim','boycott','don','deport']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfEeHRZJxLMY"
      },
      "source": [
        "# Milestone 3: Coding up our classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkvmmHOlIRG"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYOtdLcZjlkg"
      },
      "source": [
        "We have three types of information that we get from tweets: hashtags, mentions, and the actual text. As we saw, we can build lists of words that we think indicate something is a pro or anti tweet. Each list gives us a single classifier. For example, a pro hashtag classifier will see if a tweet has hashtags in our `pro_hashtags` and, if it does, it decides that the tweet is `pro refugee`. In this way, we can also build 5 classifiers other classifiers for each of our lists. Each classifier is a decision on the feature information that we care about (i.e. hashtags, mentions, or text), and which category we care to find (pro or anti). \n",
        "\n",
        "Let us build a classifier based on anti-refugee features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZxobQsLI8Wc"
      },
      "source": [
        "def anti_classifier(tweet):\n",
        "  for hashtag in tweet.hashtags:\n",
        "    if hashtag in anti_hashtags:\n",
        "      return True # for pro\n",
        "  for mention in tweet.mentions:\n",
        "    if mention in anti_mentions:\n",
        "      return True # for pro\n",
        "  for word in tweet.tokenList:\n",
        "    if word in anti_words:\n",
        "      return True # for pro    \n",
        "  return False # if none of the hastags, mentions, or words are in our anti lists, then the tweet does not express anti-refugee sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj788HFdTcUA"
      },
      "source": [
        "We need to compare our rule-based classifier's predictions with the real data. Since the `classification` value in the original data was a string, we need a helper function to convert those to Boolean values that we can use to compare to our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0sIPXevPVxR"
      },
      "source": [
        "# helper func to convert string \"TRUE\" or \"FALSE\" to boolean values\n",
        "def make_boolean(s):\n",
        "  if s.lower() == \"false\":\n",
        "    return False\n",
        "  if s.lower() == \"true\":\n",
        "    return True\n",
        "  \n",
        "# make the test data  \n",
        "correct = [make_boolean(i['classification']) for i in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3s5SIL9rCG_"
      },
      "source": [
        "Once we have made our rule based classifier, we can make predictions! Enter code in the cell below to generate a list of predictions from our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P8J5eoUqfCd"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmas0o9JsRi"
      },
      "source": [
        "**Get this classifier's accuracy below!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi5DpC1ZJnHq"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "accuracy_score(_______, ________)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quk-0sJ8KTvK"
      },
      "source": [
        "**Now try building a pro-classifier to only select for pro-refugee tweets! Then, test its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcQcEhavKeL3"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEWKxdyZsUaZ"
      },
      "source": [
        "### Exercise (Discussion): How did we do? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_cDkEwfKneB"
      },
      "source": [
        "Discuss these questions in your group, and then with your instructor:\n",
        "* How well did your rule-based classifiers perform? \n",
        "* Was there a difference when you made a classifier to select for anti-refugee sentiment vs one for pro-refugee sentiment?\n",
        "* What are pros and cons of using hand-built classifiers? \n",
        "* Are there any drawbacks to posing this question (of anti vs pro refugee sentiment) as a binary problem?"
      ]
    }
  ]
}